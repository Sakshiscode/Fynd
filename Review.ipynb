{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4bd08a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saksh\\AppData\\Local\\Temp\\ipykernel_21076\\2305305633.py:32: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  @validator('predicted_stars')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini Client Initialized.\n",
      "\n",
      "Data loaded and sampled to 5 rows.\n",
      "\n",
      " Running Zero-Shot (Baseline) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Zero-Shot (Baseline):   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\saksh\\AppData\\Local\\Temp\\ipykernel_21076\\2305305633.py:147: PydanticDeprecatedSince20: The `parse_raw` method is deprecated; if your data is JSON use `model_validate_json`, otherwise load the data then use `model_validate` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  parsed_data = RatingPrediction.parse_raw(json_response)\n",
      "Running Zero-Shot (Baseline): 100%|██████████| 5/5 [00:13<00:00,  2.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Running Few-Shot + CoT \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Few-Shot + CoT:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\saksh\\AppData\\Local\\Temp\\ipykernel_21076\\2305305633.py:147: PydanticDeprecatedSince20: The `parse_raw` method is deprecated; if your data is JSON use `model_validate_json`, otherwise load the data then use `model_validate` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  parsed_data = RatingPrediction.parse_raw(json_response)\n",
      "Running Few-Shot + CoT: 100%|██████████| 5/5 [00:05<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Running Role-Play + Format Focus \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Role-Play + Format Focus: 100%|██████████| 5/5 [00:02<00:00,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "           TASK 1: PROMPT COMPARISON RESULTS\n",
      "==================================================\n",
      "| Approach                 |   Accuracy |   JSON Validity Rate |   Reliability (Non-Empty Explanation Rate) |\n",
      "|:-------------------------|-----------:|---------------------:|-------------------------------------------:|\n",
      "| Zero-Shot (Baseline)     |        0.8 |                  1   |                                        1   |\n",
      "| Few-Shot + CoT           |        0   |                  0.2 |                                        0.2 |\n",
      "| Role-Play + Format Focus |        0   |                  0   |                                        0   |\n",
      "\n",
      "--- RESULTS SAVED ---\n",
      "✓ JSON Analysis Summary: C:\\Users\\saksh\\Downloads\\yelp.csv\\final_analysis_summary.json\n",
      "✓ Comparison Report (Markdown): C:\\Users\\saksh\\Downloads\\yelp.csv\\analysis_report.md\n",
      "✓ Full Data (CSV): C:\\Users\\saksh\\Downloads\\yelp.csv\\full_predictions_and_metrics.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from datetime import datetime, timedelta\n",
    "from pydantic import BaseModel, ValidationError, Field, validator\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from google.genai.errors import APIError\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from pathlib import Path\n",
    "import json \n",
    "\n",
    "\n",
    "\n",
    "os.environ[\"GEMINI_API_KEY\"] = \"AIzaSyAsbdGksRuXkfGpFpaV87bA87Pv1fyhiG0\" # Placeholder Key\n",
    "MODEL_NAME = \"gemini-2.5-flash\" \n",
    "\n",
    "try:\n",
    "    client = genai.Client()\n",
    "    print(\"Gemini Client Initialized.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing Gemini client.\")\n",
    "    client = None\n",
    "\n",
    "\n",
    "class RatingPrediction(BaseModel):\n",
    "    \n",
    "    predicted_stars: int = Field(..., description=\"The predicted star rating from 1 to 5.\")\n",
    "    explanation: str = Field(..., description=\"A brief explanation for the assigned rating.\")\n",
    "    \n",
    "    @validator('predicted_stars')\n",
    "    def check_star_range(cls, v):\n",
    "        if not (1 <= v <= 5):\n",
    "            raise ValueError(f'predicted_stars must be between 1 and 5, received {v}')\n",
    "        return v\n",
    "\n",
    "#DATA LOADING AND SAMPLING\n",
    "\n",
    "df_sampled = pd.DataFrame()\n",
    "\n",
    "DATA_PATH = r\"C:\\Users\\saksh\\Downloads\\yelp.csv\\yelp.csv\" \n",
    "SAMPLE_SIZE = 5 \n",
    "\n",
    "try:\n",
    "    df_full = pd.read_csv(DATA_PATH)\n",
    "    \n",
    "    df_sampled = (\n",
    "        df_full[['text', 'stars']]\n",
    "        .dropna()\n",
    "        .sample(SAMPLE_SIZE, random_state=42)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    \n",
    "    df_sampled = df_sampled.rename(columns={'stars': 'actual_stars'})\n",
    "\n",
    "    print(f\"\\nData loaded and sampled to {len(df_sampled)} rows.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"\\nError: Dataset file not found at {DATA_PATH}. Please check the path and file name.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError during data loading/sampling: {e}\")\n",
    "\n",
    "\n",
    "# PROMPT APPROACHES\n",
    "## A. Approach 1: Zero-Shot\n",
    "PROMPT_1_ZERO_SHOT = \"\"\"\n",
    "You are an expert review classifier. Your task is to analyze the following Yelp review and classify it into a star rating from 1 (worst) to 5 (best).\n",
    "You MUST return your response as a valid JSON object matching the provided schema. Do not include any text outside of the JSON object.\n",
    "REVIEW: \"{review_text}\"\n",
    "\"\"\"\n",
    "\n",
    "## B. Approach 2: Few-Shot with CoT (Chain-of-Thought)\n",
    "FEW_SHOT_EXAMPLE = \"\"\"\n",
    "EXAMPLE REVIEW: \"The waiter was rude and spilled coffee on my laptop. The food was mediocre and took an hour to arrive. I will never return here.\"\n",
    "EXAMPLE RATING: 1\n",
    "EXAMPLE EXPLANATION: The review contains multiple strong negative indicators such as 'rude', 'spilled coffee', 'mediocre food', and 'took an hour', all pointing to a terrible experience.\n",
    "\"\"\"\n",
    "PROMPT_2_FEW_SHOT_COT = f\"\"\"\n",
    "You are an expert review classifier. Your task is to analyze the following Yelp review and classify it into a star rating from 1 (worst) to 5 (best).\n",
    "Here is an example to guide your classification:\n",
    "---\n",
    "{FEW_SHOT_EXAMPLE}\n",
    "---\n",
    "Now, classify the following review. First, briefly mention the key sentiment drivers (e.g., positive keywords, negative experience) in a thought process.\n",
    "You MUST return your final response as a valid JSON object matching the provided schema.\n",
    "REVIEW: \"{{review_text}}\"\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "## C. Approach 3: Role-Play and Format-Focused\n",
    "PROMPT_3_ROLE_FOCUSED = \"\"\"\n",
    "SYSTEM INSTRUCTION: You are a **highly reliable and meticulous Sentiment Analysis Bot** dedicated to classifying customer reviews with perfect JSON adherence.\n",
    "Your sole output must be a JSON object, and you must verify that the 'predicted_stars' is an integer between 1 and 5, and the 'explanation' is a brief reasoning.\n",
    "Review to Analyze:\n",
    "---\n",
    "{{review_text}}\n",
    "---\n",
    "\"\"\"\n",
    "\n",
    "PROMPT_MAP = {\n",
    "    \"Zero-Shot (Baseline)\": PROMPT_1_ZERO_SHOT,\n",
    "    \"Few-Shot + CoT\": PROMPT_2_FEW_SHOT_COT,\n",
    "    \"Role-Play + Format Focus\": PROMPT_3_ROLE_FOCUSED,\n",
    "}\n",
    "\n",
    "\n",
    "def classify_reviews_with_llm(df: pd.DataFrame, prompt_template: str, prompt_name: str) -> Tuple[Dict[str, Any], pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Processes the sampled dataframe using a given prompt and evaluates the results.\n",
    "    \"\"\"\n",
    "    if client is None:\n",
    "        raise RuntimeError(\"Gemini Client failed to initialize. Cannot run API calls.\")\n",
    "        \n",
    "    \n",
    "    df['llm_output_raw'] = None\n",
    "    df['predicted_stars'] = None\n",
    "    df['explanation'] = None\n",
    "    \n",
    "    valid_json_count = 0\n",
    "    \n",
    "    \n",
    "    config = types.GenerateContentConfig(\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_schema=RatingPrediction,\n",
    "    )\n",
    "\n",
    "    \n",
    "    for index, row in tqdm(df.iterrows(), total=len(df), desc=f\"Running {prompt_name}\"):\n",
    "        review_text = row['text']\n",
    "        \n",
    "        \n",
    "        prompt = prompt_template.format(review_text=review_text)\n",
    "        \n",
    "        try:\n",
    "            response = client.models.generate_content(\n",
    "                model=MODEL_NAME,\n",
    "                contents=[prompt],\n",
    "                config=config,\n",
    "            )\n",
    "            \n",
    "            json_response = response.text\n",
    "            df.at[index, 'llm_output_raw'] = json_response\n",
    "\n",
    "            \n",
    "            try:\n",
    "                parsed_data = RatingPrediction.parse_raw(json_response)\n",
    "                \n",
    "                df.at[index, 'predicted_stars'] = parsed_data.predicted_stars\n",
    "                df.at[index, 'explanation'] = parsed_data.explanation\n",
    "                valid_json_count += 1\n",
    "\n",
    "            except (json.JSONDecodeError, ValidationError) as e:\n",
    "                \n",
    "                df.at[index, 'llm_output_raw'] = f\"JSON/Validation Error: {json_response} | {e}\"\n",
    "                pass \n",
    "\n",
    "        except APIError as e:\n",
    "            df.at[index, 'llm_output_raw'] = f\"API Error: {e}\"\n",
    "        \n",
    "    \n",
    "    df_results = df.dropna(subset=['predicted_stars']).copy()\n",
    "    if not df_results.empty:\n",
    "        accuracy = accuracy_score(df_results['actual_stars'].astype(int), df_results['predicted_stars'].astype(int))\n",
    "    else:\n",
    "        accuracy = 0.0\n",
    "\n",
    "    json_validity_rate = valid_json_count / len(df)\n",
    "    non_empty_explanation_count = df_results['explanation'].apply(lambda x: bool(x) and str(x).strip() != '').sum()\n",
    "    explanation_rate = non_empty_explanation_count / len(df)\n",
    "    \n",
    "    metrics = {\n",
    "        \"Approach\": prompt_name,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"JSON Validity Rate\": json_validity_rate,\n",
    "        \"Reliability (Non-Empty Explanation Rate)\": explanation_rate\n",
    "    }\n",
    "    \n",
    "    return metrics, df.copy()\n",
    "\n",
    "# Comparing\n",
    "\n",
    "def main():\n",
    "    results_list = []\n",
    "    all_results = {}\n",
    "\n",
    "    OUTPUT_DIR = Path(r\"C:\\Users\\saksh\\Downloads\\yelp.csv\")\n",
    "    OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "    global df_sampled \n",
    "\n",
    "    if df_sampled.empty:\n",
    "        print(\"\\nSampled DataFrame is empty.\")\n",
    "        return\n",
    "\n",
    "    for name, template in PROMPT_MAP.items():\n",
    "        print(f\"\\n Running {name} \")\n",
    "        df_to_process = df_sampled[['text', 'actual_stars']].copy()\n",
    "        \n",
    "        metrics, df_output = classify_reviews_with_llm(df_to_process, template, name)\n",
    "        \n",
    "        results_list.append(metrics)\n",
    "        all_results[name] = df_output\n",
    "\n",
    "    df_comparison = pd.DataFrame(results_list).set_index('Approach')\n",
    "    \n",
    "    best_approach = df_comparison['Accuracy'].idxmax()\n",
    "    combined_df = pd.concat([df.assign(Approach=name) for name, df in all_results.items()])\n",
    "    final_output_structure = {\n",
    "        \"metadata\": {\n",
    "            \"model_name\": MODEL_NAME,\n",
    "            \"sample_size\": SAMPLE_SIZE,\n",
    "            \"date\": datetime.now().isoformat()\n",
    "        },\n",
    "        \"comparison_metrics\": df_comparison.reset_index().to_dict(orient='records'),\n",
    "        \"best_approach\": best_approach,\n",
    "        \"full_results_data\": combined_df.to_dict(orient='records')\n",
    "    }\n",
    "    \n",
    "    json_file_path = OUTPUT_DIR / \"final_analysis_summary.json\"\n",
    "    with open(json_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(final_output_structure, f, indent=2, default=str)\n",
    "\n",
    "\n",
    "    markdown_content = []\n",
    "    markdown_content.append(\"# Gemini Prompt Comparison Analysis\\n\")\n",
    "    markdown_content.append(\"## 1. Metric Comparison\\n\")\n",
    "    markdown_content.append(df_comparison.to_markdown())\n",
    "    markdown_content.append(\"\\n\\n---\\n\")\n",
    "\n",
    "    markdown_content.append(f\"## 2. Detailed Results for Best Approach: {best_approach}\\n\")\n",
    "    markdown_content.append(\"\\nExample 5 Predictions from the Best Approach:\\n\")\n",
    "    report_df = all_results[best_approach][['text', 'actual_stars', 'predicted_stars', 'explanation']].head()\n",
    "    markdown_content.append(report_df.to_markdown(index=False))\n",
    "\n",
    "    report_file_path = OUTPUT_DIR / \"analysis_report.md\"\n",
    "    with open(report_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.writelines(markdown_content)\n",
    "        \n",
    "    full_df_path = OUTPUT_DIR / \"full_predictions_and_metrics.csv\"\n",
    "    combined_df.to_csv(full_df_path, index=False)\n",
    "    \n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"           TASK 1: PROMPT COMPARISON RESULTS\")\n",
    "    print(\"=\"*50)\n",
    "    print(df_comparison.to_markdown())\n",
    "    print(\"\\n--- RESULTS SAVED ---\")\n",
    "    print(f\"✓ JSON Analysis Summary: {json_file_path.resolve()}\")\n",
    "    print(f\"✓ Comparison Report (Markdown): {report_file_path.resolve()}\")\n",
    "    print(f\"✓ Full Data (CSV): {full_df_path.resolve()}\")\n",
    "\n",
    "    return df_comparison\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
