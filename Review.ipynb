{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8389cf2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini Client Initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saksh\\AppData\\Local\\Temp\\ipykernel_21076\\1402381211.py:32: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  @validator('predicted_stars')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data loaded and sampled to 10 rows.\n",
      "\n",
      "--- Running Zero-Shot (Baseline) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Zero-Shot (Baseline):   0%|          | 0/10 [00:00<?, ?it/s]C:\\Users\\saksh\\AppData\\Local\\Temp\\ipykernel_21076\\1402381211.py:155: PydanticDeprecatedSince20: The `parse_raw` method is deprecated; if your data is JSON use `model_validate_json`, otherwise load the data then use `model_validate` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  parsed_data = RatingPrediction.parse_raw(json_response)\n",
      "Running Zero-Shot (Baseline): 100%|██████████| 10/10 [00:30<00:00,  3.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Few-Shot + CoT ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Few-Shot + CoT:   0%|          | 0/10 [00:00<?, ?it/s]C:\\Users\\saksh\\AppData\\Local\\Temp\\ipykernel_21076\\1402381211.py:155: PydanticDeprecatedSince20: The `parse_raw` method is deprecated; if your data is JSON use `model_validate_json`, otherwise load the data then use `model_validate` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  parsed_data = RatingPrediction.parse_raw(json_response)\n",
      "Running Few-Shot + CoT: 100%|██████████| 10/10 [00:07<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Role-Play + Format Focus ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Role-Play + Format Focus: 100%|██████████| 10/10 [00:05<00:00,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "           TASK 1: PROMPT COMPARISON RESULTS\n",
      "==================================================\n",
      "| Approach                 |   Accuracy |   JSON Validity Rate |   Reliability (Non-Empty Explanation Rate) |\n",
      "|:-------------------------|-----------:|---------------------:|-------------------------------------------:|\n",
      "| Zero-Shot (Baseline)     |        0.6 |                  1   |                                        1   |\n",
      "| Few-Shot + CoT           |        0   |                  0.1 |                                        0.1 |\n",
      "| Role-Play + Format Focus |        0   |                  0   |                                        0   |\n",
      "\n",
      "\n",
      "--- Report Content Examples (Best Approach: Zero-Shot (Baseline)) ---\n",
      "\n",
      "Example 5 Predictions from the Best Approach:\n",
      "| text                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |   actual_stars |   predicted_stars | explanation                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------:|------------------:|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| We got here around midnight last Friday... the place was dead. However, they were still serving food and we enjoyed some well made pub grub. Service was friendly, quality cocktails were served, and the atmosphere is derived from an old Uno's, which certainly works for a sports bar. It being located in a somewhat commercial area, I can see why it's empty so late on a Friday. From what my friends tell me - this is a great spot for happy hour, and it stays relatively busy thru 10pm. |              4 |                 5 | The reviewer praises the well-made pub grub, friendly service, quality cocktails, and suitable sports bar atmosphere. The update highlights a great patio for day-drinking, and the initial observation of it being empty late at night is contextualized.                                                                                                                                                                |\n",
      "|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |                |                   |                                                                                                                                                                                                                                                                                                                                                                                                                           |\n",
      "| *UPDATE - Great patio for day-drinking on the weekends!                                                                                                                                                                                                                                                                                                                                                                                                                                              |                |                   |                                                                                                                                                                                                                                                                                                                                                                                                                           |\n",
      "| Brought a friend from Louisiana here.  She says that the crawfish etouffee here is the best she's had outside of Louisiana!                                                                                                                                                                                                                                                                                                                                                                          |              5 |                 5 | The review indicates exceptionally high quality for the crawfish etouffee, as praised by a friend from Louisiana who considers it the best she's had outside her home state.                                                                                                                                                                                                                                              |\n",
      "| Every friday, my dad and I eat here. We order the same thing... pizza wraps. they are good and fillng. iced tea hits the spot. Typical strip mall pizza.                                                                                                                                                                                                                                                                                                                                             |              3 |                 4 | The review indicates repeat patronage ('Every Friday, my dad and I eat here') and describes the food as 'good and filling' and the iced tea as 'hits the spot', which are strong positive indicators. While 'Typical strip mall pizza' suggests it's not extraordinary, it doesn't detract significantly from the overall positive experience and satisfaction expressed by the regular visits and enjoyment of the food. |\n",
      "| My husband and I were really, really disappointed with our experience at Coulter Infiniti.  We brought our 2006 G37 with barely 34,000 miles (and out of warranty) to have the moon roof looked at.  The moon roof wouldn't open or close, but you could hear the motor working.                                                                                                                                                                                                                     |              1 |                 1 | The review expresses extreme disappointment and distrust, accusing the dealership of recommending an unnecessary and expensive $3000 repair when the actual issue was minor and cost only $100 at another shop. The customer felt misled and is now avoiding the brand entirely.                                                                                                                                          |\n",
      "|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |                |                   |                                                                                                                                                                                                                                                                                                                                                                                                                           |\n",
      "| After leaving the car there all day we get a call that the car needs a whole new moon roof track - which would cost over $3000.  We contacted their corporate office to attempt to get some or all of the cost covered, due to the fact that it's a big repair and the car  is barely out of warranty.  We were flat out denied.  Twice.                                                                                                                                                             |                |                   |                                                                                                                                                                                                                                                                                                                                                                                                                           |\n",
      "|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |                |                   |                                                                                                                                                                                                                                                                                                                                                                                                                           |\n",
      "| We were dreading the big repair, and decided to bring the car to a different auto shop (which will get a big, fat 5 star review) for a second opinion.  Turns out the track just had a BUILD UP OF DIRT AND DEBRIS and needed to be cleaned out.  Seriously?!?!  Just over $100 later we were on our way!                                                                                                                                                                                            |                |                   |                                                                                                                                                                                                                                                                                                                                                                                                                           |\n",
      "|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |                |                   |                                                                                                                                                                                                                                                                                                                                                                                                                           |\n",
      "| I find it VERY hard to believe that Coulter didn't know this was the problem.  We are now searching for a new SUV, and will not be choosing Infiniti.  How disappointing.                                                                                                                                                                                                                                                                                                                            |                |                   |                                                                                                                                                                                                                                                                                                                                                                                                                           |\n",
      "| Love this place!  Was in phoenix 3 weeks for work.  Ate here 3 times.  The squid salad is amazing!  They also have sushi, bibimbap, and tons of other items.  Must try!                                                                                                                                                                                                                                                                                                                              |              5 |                 5 | The review expresses strong positive sentiment, repeat patronage, specific praise for a dish ('amazing'), and a clear recommendation ('Must try!'), all indicating an excellent experience.                                                                                                                                                                                                                               |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from pydantic import BaseModel, ValidationError, Field, validator\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from google.genai.errors import APIError\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "# ============================================================================\n",
    "# --- 1. SETUP AND CONFIGURATION ---\n",
    "# ============================================================================\n",
    "\n",
    "# 1.1. Set up Gemini API Key (NOTE: Placeholder key used for structure)\n",
    "os.environ[\"GEMINI_API_KEY\"] = \"AIzaSyDEuzSUl2PUYHjPcEB5qJQpMZEuy4FNvvI\" # Placeholder Key\n",
    "MODEL_NAME = \"gemini-2.5-flash\" \n",
    "\n",
    "try:\n",
    "    client = genai.Client()\n",
    "    print(\"Gemini Client Initialized.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing Gemini client. Make sure the API key is correct. Error: {e}\")\n",
    "    client = None\n",
    "\n",
    "# 1.2. Define the REQUIRED Structured Output Schema using Pydantic\n",
    "class RatingPrediction(BaseModel):\n",
    "    \"\"\"Schema for the LLM's structured output.\"\"\"\n",
    "    predicted_stars: int = Field(..., description=\"The predicted star rating from 1 to 5.\")\n",
    "    explanation: str = Field(..., description=\"A brief explanation for the predicted rating.\")\n",
    "    \n",
    "    @validator('predicted_stars')\n",
    "    def check_star_range(cls, v):\n",
    "        if not (1 <= v <= 5):\n",
    "            raise ValueError(f'predicted_stars must be between 1 and 5, received {v}')\n",
    "        return v\n",
    "\n",
    "# --- 2. DATA LOADING AND SAMPLING ---\n",
    "\n",
    "# CRITICAL: Initialize df_sampled globally as an empty DataFrame to prevent NameError\n",
    "df_sampled = pd.DataFrame()\n",
    "\n",
    "DATA_PATH = r\"C:\\Users\\saksh\\Downloads\\yelp.csv\\yelp.csv\" \n",
    "SAMPLE_SIZE = 10 \n",
    "\n",
    "try:\n",
    "    df_full = pd.read_csv(DATA_PATH)\n",
    "    \n",
    "    # Filter, sample, and rename columns for evaluation clarity\n",
    "    df_sampled = (\n",
    "        df_full[['text', 'stars']]\n",
    "        .dropna()\n",
    "        .sample(SAMPLE_SIZE, random_state=42)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    \n",
    "    df_sampled = df_sampled.rename(columns={'stars': 'actual_stars'})\n",
    "\n",
    "    print(f\"\\nData loaded and sampled to {len(df_sampled)} rows.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"\\nError: Dataset file not found at {DATA_PATH}. Please check the path and file name.\")\n",
    "    # df_sampled remains empty DataFrame, preventing NameError later\n",
    "except Exception as e:\n",
    "    print(f\"\\nError during data loading/sampling: {e}\")\n",
    "    # df_sampled remains empty DataFrame\n",
    "# --- Data Loading End ---\n",
    "\n",
    "\n",
    "# --- 3. PROMPT APPROACH DEFINITIONS (3 VERSIONS REQUIRED) ---\n",
    "\n",
    "## A. Approach 1: Zero-Shot (Baseline)\n",
    "PROMPT_1_ZERO_SHOT = \"\"\"\n",
    "You are an expert review classifier. Your task is to analyze the following Yelp review and classify it into a star rating from 1 (worst) to 5 (best).\n",
    "You MUST return your response as a valid JSON object matching the provided schema. Do not include any text outside of the JSON object.\n",
    "REVIEW: \"{review_text}\"\n",
    "\"\"\"\n",
    "\n",
    "## B. Approach 2: Few-Shot with CoT (Chain-of-Thought)\n",
    "FEW_SHOT_EXAMPLE = \"\"\"\n",
    "EXAMPLE REVIEW: \"The waiter was rude and spilled coffee on my laptop. The food was mediocre and took an hour to arrive. I will never return here.\"\n",
    "EXAMPLE RATING: 1\n",
    "EXAMPLE EXPLANATION: The review contains multiple strong negative indicators such as 'rude', 'spilled coffee', 'mediocre food', and 'took an hour', all pointing to a terrible experience.\n",
    "\"\"\"\n",
    "PROMPT_2_FEW_SHOT_COT = f\"\"\"\n",
    "You are an expert review classifier. Your task is to analyze the following Yelp review and classify it into a star rating from 1 (worst) to 5 (best).\n",
    "Here is an example to guide your classification:\n",
    "---\n",
    "{FEW_SHOT_EXAMPLE}\n",
    "---\n",
    "Now, classify the following review. First, briefly mention the key sentiment drivers (e.g., positive keywords, negative experience) in a thought process.\n",
    "You MUST return your final response as a valid JSON object matching the provided schema.\n",
    "REVIEW: \"{{review_text}}\"\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "## C. Approach 3: Role-Play and Format-Focused\n",
    "PROMPT_3_ROLE_FOCUSED = \"\"\"\n",
    "SYSTEM INSTRUCTION: You are a **highly reliable and meticulous Sentiment Analysis Bot** dedicated to classifying customer reviews with perfect JSON adherence.\n",
    "Your sole output must be a JSON object, and you must verify that the 'predicted_stars' is an integer between 1 and 5, and the 'explanation' is a brief reasoning.\n",
    "Review to Analyze:\n",
    "---\n",
    "{{review_text}}\n",
    "---\n",
    "\"\"\"\n",
    "\n",
    "PROMPT_MAP = {\n",
    "    \"Zero-Shot (Baseline)\": PROMPT_1_ZERO_SHOT,\n",
    "    \"Few-Shot + CoT\": PROMPT_2_FEW_SHOT_COT,\n",
    "    \"Role-Play + Format Focus\": PROMPT_3_ROLE_FOCUSED,\n",
    "}\n",
    "\n",
    "# --- 4. CORE FUNCTION: LLM CLASSIFICATION AND EVALUATION ---\n",
    "\n",
    "def classify_reviews_with_llm(df: pd.DataFrame, prompt_template: str, prompt_name: str) -> Tuple[Dict[str, Any], pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Processes the sampled dataframe using a given prompt and evaluates the results.\n",
    "    \"\"\"\n",
    "    if client is None:\n",
    "        raise RuntimeError(\"Gemini Client failed to initialize. Cannot run API calls.\")\n",
    "        \n",
    "    # Initialize columns for LLM results\n",
    "    df['llm_output_raw'] = None\n",
    "    df['predicted_stars'] = None\n",
    "    df['explanation'] = None\n",
    "    \n",
    "    valid_json_count = 0\n",
    "    \n",
    "    # Configure the request to use the Pydantic schema for structured output\n",
    "    config = types.GenerateContentConfig(\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_schema=RatingPrediction,\n",
    "    )\n",
    "\n",
    "    # 4.1. Iterate through the sampled reviews\n",
    "    for index, row in tqdm(df.iterrows(), total=len(df), desc=f\"Running {prompt_name}\"):\n",
    "        review_text = row['text']\n",
    "        \n",
    "        # 4.2. Format the prompt and send the request\n",
    "        prompt = prompt_template.format(review_text=review_text)\n",
    "        \n",
    "        try:\n",
    "            response = client.models.generate_content(\n",
    "                model=MODEL_NAME,\n",
    "                contents=[prompt],\n",
    "                config=config,\n",
    "            )\n",
    "            \n",
    "            json_response = response.text\n",
    "            df.at[index, 'llm_output_raw'] = json_response\n",
    "\n",
    "            # 4.3. Parse and Validate the JSON output\n",
    "            try:\n",
    "                # Use Pydantic's parse_raw to validate the JSON against the schema\n",
    "                parsed_data = RatingPrediction.parse_raw(json_response)\n",
    "                \n",
    "                df.at[index, 'predicted_stars'] = parsed_data.predicted_stars\n",
    "                df.at[index, 'explanation'] = parsed_data.explanation\n",
    "                valid_json_count += 1\n",
    "\n",
    "            except (json.JSONDecodeError, ValidationError) as e:\n",
    "                # JSON invalidity or schema violation\n",
    "                df.at[index, 'llm_output_raw'] = f\"JSON/Validation Error: {json_response} | {e}\"\n",
    "                pass # Values remain None\n",
    "\n",
    "        except APIError as e:\n",
    "            # Handle API errors \n",
    "            df.at[index, 'llm_output_raw'] = f\"API Error: {e}\"\n",
    "        \n",
    "    # 4.4. Calculate Metrics\n",
    "    \n",
    "    df_results = df.dropna(subset=['predicted_stars']).copy()\n",
    "    if not df_results.empty:\n",
    "        accuracy = accuracy_score(df_results['actual_stars'].astype(int), df_results['predicted_stars'].astype(int))\n",
    "    else:\n",
    "        accuracy = 0.0\n",
    "\n",
    "    json_validity_rate = valid_json_count / len(df)\n",
    "    non_empty_explanation_count = df_results['explanation'].apply(lambda x: bool(x) and str(x).strip() != '').sum()\n",
    "    explanation_rate = non_empty_explanation_count / len(df)\n",
    "    \n",
    "    metrics = {\n",
    "        \"Approach\": prompt_name,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"JSON Validity Rate\": json_validity_rate,\n",
    "        \"Reliability (Non-Empty Explanation Rate)\": explanation_rate\n",
    "    }\n",
    "    \n",
    "    return metrics, df.copy()\n",
    "\n",
    "# --- 5. EXECUTE ALL APPROACHES AND COMPARE ---\n",
    "\n",
    "def main():\n",
    "    results_list = []\n",
    "    all_results = {}\n",
    "\n",
    "    global df_sampled \n",
    "\n",
    "    if df_sampled.empty:\n",
    "        print(\"\\nCannot run evaluation: Sampled DataFrame is empty. Check your DATA_PATH.\")\n",
    "        return\n",
    "\n",
    "    for name, template in PROMPT_MAP.items():\n",
    "        print(f\"\\n--- Running {name} ---\")\n",
    "        \n",
    "        # Make a deep copy of the sampled data for each independent run\n",
    "        df_to_process = df_sampled[['text', 'actual_stars']].copy()\n",
    "        \n",
    "        metrics, df_output = classify_reviews_with_llm(df_to_process, template, name)\n",
    "        \n",
    "        results_list.append(metrics)\n",
    "        all_results[name] = df_output\n",
    "\n",
    "    # Create the final comparison table\n",
    "    df_comparison = pd.DataFrame(results_list).set_index('Approach')\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"           TASK 1: PROMPT COMPARISON RESULTS\")\n",
    "    print(\"=\"*50)\n",
    "    print(df_comparison.to_markdown())\n",
    "\n",
    "    # --- 6. REQUIRED REPORTING OUTPUTS ---\n",
    "\n",
    "    best_approach = df_comparison['Accuracy'].idxmax()\n",
    "    print(f\"\\n\\n--- Report Content Examples (Best Approach: {best_approach}) ---\")\n",
    "    print(\"\\nExample 5 Predictions from the Best Approach:\")\n",
    "    \n",
    "    report_df = all_results[best_approach][['text', 'actual_stars', 'predicted_stars', 'explanation']].head()\n",
    "    print(report_df.to_markdown(index=False))\n",
    "\n",
    "    return df_comparison\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4bd08a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini Client Initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saksh\\AppData\\Local\\Temp\\ipykernel_21076\\264293846.py:35: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  @validator('predicted_stars')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data loaded and sampled to 5 rows.\n",
      "\n",
      "--- Running Zero-Shot (Baseline) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Zero-Shot (Baseline):   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\saksh\\AppData\\Local\\Temp\\ipykernel_21076\\264293846.py:153: PydanticDeprecatedSince20: The `parse_raw` method is deprecated; if your data is JSON use `model_validate_json`, otherwise load the data then use `model_validate` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  parsed_data = RatingPrediction.parse_raw(json_response)\n",
      "Running Zero-Shot (Baseline): 100%|██████████| 5/5 [00:16<00:00,  3.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Few-Shot + CoT ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Few-Shot + CoT:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\saksh\\AppData\\Local\\Temp\\ipykernel_21076\\264293846.py:153: PydanticDeprecatedSince20: The `parse_raw` method is deprecated; if your data is JSON use `model_validate_json`, otherwise load the data then use `model_validate` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  parsed_data = RatingPrediction.parse_raw(json_response)\n",
      "Running Few-Shot + CoT: 100%|██████████| 5/5 [00:04<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Role-Play + Format Focus ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Role-Play + Format Focus: 100%|██████████| 5/5 [00:02<00:00,  2.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "           TASK 1: PROMPT COMPARISON RESULTS\n",
      "==================================================\n",
      "| Approach                 |   Accuracy |   JSON Validity Rate |   Reliability (Non-Empty Explanation Rate) |\n",
      "|:-------------------------|-----------:|---------------------:|-------------------------------------------:|\n",
      "| Zero-Shot (Baseline)     |        0.8 |                  1   |                                        1   |\n",
      "| Few-Shot + CoT           |        0   |                  0.2 |                                        0.2 |\n",
      "| Role-Play + Format Focus |        0   |                  0   |                                        0   |\n",
      "\n",
      "--- RESULTS SAVED ---\n",
      "✓ JSON Analysis Summary: C:\\Users\\saksh\\Downloads\\yelp.csv\\final_analysis_summary.json\n",
      "✓ Comparison Report (Markdown): C:\\Users\\saksh\\Downloads\\yelp.csv\\analysis_report.md\n",
      "✓ Full Data (CSV): C:\\Users\\saksh\\Downloads\\yelp.csv\\full_predictions_and_metrics.csv\n",
      "---------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from datetime import datetime, timedelta\n",
    "from pydantic import BaseModel, ValidationError, Field, validator\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from google.genai.errors import APIError\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from pathlib import Path\n",
    "import json # Ensure json is imported\n",
    "\n",
    "# ============================================================================\n",
    "# --- 1. SETUP AND CONFIGURATION ---\n",
    "# ============================================================================\n",
    "\n",
    "# 1.1. Set up Gemini API Key (NOTE: Placeholder key used for structure)\n",
    "os.environ[\"GEMINI_API_KEY\"] = \"AIzaSyDEuzSUl2PUYHjPcEB5qJQpMZEuy4FNvvI\" # Placeholder Key\n",
    "MODEL_NAME = \"gemini-2.5-flash\" \n",
    "\n",
    "try:\n",
    "    client = genai.Client()\n",
    "    print(\"Gemini Client Initialized.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing Gemini client. Make sure the API key is correct. Error: {e}\")\n",
    "    client = None\n",
    "\n",
    "# 1.2. Define the REQUIRED Structured Output Schema using Pydantic\n",
    "class RatingPrediction(BaseModel):\n",
    "    \"\"\"Schema for the LLM's structured output.\"\"\"\n",
    "    predicted_stars: int = Field(..., description=\"The predicted star rating from 1 to 5.\")\n",
    "    explanation: str = Field(..., description=\"A brief explanation for the assigned rating.\")\n",
    "    \n",
    "    @validator('predicted_stars')\n",
    "    def check_star_range(cls, v):\n",
    "        if not (1 <= v <= 5):\n",
    "            raise ValueError(f'predicted_stars must be between 1 and 5, received {v}')\n",
    "        return v\n",
    "\n",
    "# --- 2. DATA LOADING AND SAMPLING ---\n",
    "\n",
    "# CRITICAL: Initialize df_sampled globally as an empty DataFrame to prevent NameError\n",
    "df_sampled = pd.DataFrame()\n",
    "\n",
    "DATA_PATH = r\"C:\\Users\\saksh\\Downloads\\yelp.csv\\yelp.csv\" \n",
    "SAMPLE_SIZE = 5 \n",
    "\n",
    "try:\n",
    "    df_full = pd.read_csv(DATA_PATH)\n",
    "    \n",
    "    # Filter, sample, and rename columns for evaluation clarity\n",
    "    df_sampled = (\n",
    "        df_full[['text', 'stars']]\n",
    "        .dropna()\n",
    "        .sample(SAMPLE_SIZE, random_state=42)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    \n",
    "    df_sampled = df_sampled.rename(columns={'stars': 'actual_stars'})\n",
    "\n",
    "    print(f\"\\nData loaded and sampled to {len(df_sampled)} rows.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"\\nError: Dataset file not found at {DATA_PATH}. Please check the path and file name.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError during data loading/sampling: {e}\")\n",
    "\n",
    "\n",
    "# --- 3. PROMPT APPROACH DEFINITIONS (3 VERSIONS REQUIRED) ---\n",
    "## A. Approach 1: Zero-Shot (Baseline)\n",
    "PROMPT_1_ZERO_SHOT = \"\"\"\n",
    "You are an expert review classifier. Your task is to analyze the following Yelp review and classify it into a star rating from 1 (worst) to 5 (best).\n",
    "You MUST return your response as a valid JSON object matching the provided schema. Do not include any text outside of the JSON object.\n",
    "REVIEW: \"{review_text}\"\n",
    "\"\"\"\n",
    "\n",
    "## B. Approach 2: Few-Shot with CoT (Chain-of-Thought)\n",
    "FEW_SHOT_EXAMPLE = \"\"\"\n",
    "EXAMPLE REVIEW: \"The waiter was rude and spilled coffee on my laptop. The food was mediocre and took an hour to arrive. I will never return here.\"\n",
    "EXAMPLE RATING: 1\n",
    "EXAMPLE EXPLANATION: The review contains multiple strong negative indicators such as 'rude', 'spilled coffee', 'mediocre food', and 'took an hour', all pointing to a terrible experience.\n",
    "\"\"\"\n",
    "PROMPT_2_FEW_SHOT_COT = f\"\"\"\n",
    "You are an expert review classifier. Your task is to analyze the following Yelp review and classify it into a star rating from 1 (worst) to 5 (best).\n",
    "Here is an example to guide your classification:\n",
    "---\n",
    "{FEW_SHOT_EXAMPLE}\n",
    "---\n",
    "Now, classify the following review. First, briefly mention the key sentiment drivers (e.g., positive keywords, negative experience) in a thought process.\n",
    "You MUST return your final response as a valid JSON object matching the provided schema.\n",
    "REVIEW: \"{{review_text}}\"\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "## C. Approach 3: Role-Play and Format-Focused\n",
    "PROMPT_3_ROLE_FOCUSED = \"\"\"\n",
    "SYSTEM INSTRUCTION: You are a **highly reliable and meticulous Sentiment Analysis Bot** dedicated to classifying customer reviews with perfect JSON adherence.\n",
    "Your sole output must be a JSON object, and you must verify that the 'predicted_stars' is an integer between 1 and 5, and the 'explanation' is a brief reasoning.\n",
    "Review to Analyze:\n",
    "---\n",
    "{{review_text}}\n",
    "---\n",
    "\"\"\"\n",
    "\n",
    "PROMPT_MAP = {\n",
    "    \"Zero-Shot (Baseline)\": PROMPT_1_ZERO_SHOT,\n",
    "    \"Few-Shot + CoT\": PROMPT_2_FEW_SHOT_COT,\n",
    "    \"Role-Play + Format Focus\": PROMPT_3_ROLE_FOCUSED,\n",
    "}\n",
    "# --- 4. CORE FUNCTION: LLM CLASSIFICATION AND EVALUATION ---\n",
    "\n",
    "def classify_reviews_with_llm(df: pd.DataFrame, prompt_template: str, prompt_name: str) -> Tuple[Dict[str, Any], pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Processes the sampled dataframe using a given prompt and evaluates the results.\n",
    "    \"\"\"\n",
    "    if client is None:\n",
    "        raise RuntimeError(\"Gemini Client failed to initialize. Cannot run API calls.\")\n",
    "        \n",
    "    # Initialize columns for LLM results\n",
    "    df['llm_output_raw'] = None\n",
    "    df['predicted_stars'] = None\n",
    "    df['explanation'] = None\n",
    "    \n",
    "    valid_json_count = 0\n",
    "    \n",
    "    # Configure the request to use the Pydantic schema for structured output\n",
    "    config = types.GenerateContentConfig(\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_schema=RatingPrediction,\n",
    "    )\n",
    "\n",
    "    # 4.1. Iterate through the sampled reviews\n",
    "    for index, row in tqdm(df.iterrows(), total=len(df), desc=f\"Running {prompt_name}\"):\n",
    "        review_text = row['text']\n",
    "        \n",
    "        # 4.2. Format the prompt and send the request\n",
    "        prompt = prompt_template.format(review_text=review_text)\n",
    "        \n",
    "        try:\n",
    "            response = client.models.generate_content(\n",
    "                model=MODEL_NAME,\n",
    "                contents=[prompt],\n",
    "                config=config,\n",
    "            )\n",
    "            \n",
    "            json_response = response.text\n",
    "            df.at[index, 'llm_output_raw'] = json_response\n",
    "\n",
    "            # 4.3. Parse and Validate the JSON output\n",
    "            try:\n",
    "                # Use Pydantic's parse_raw to validate the JSON against the schema\n",
    "                parsed_data = RatingPrediction.parse_raw(json_response)\n",
    "                \n",
    "                df.at[index, 'predicted_stars'] = parsed_data.predicted_stars\n",
    "                df.at[index, 'explanation'] = parsed_data.explanation\n",
    "                valid_json_count += 1\n",
    "\n",
    "            except (json.JSONDecodeError, ValidationError) as e:\n",
    "                # JSON invalidity or schema violation\n",
    "                df.at[index, 'llm_output_raw'] = f\"JSON/Validation Error: {json_response} | {e}\"\n",
    "                pass # Values remain None\n",
    "\n",
    "        except APIError as e:\n",
    "            # Handle API errors \n",
    "            df.at[index, 'llm_output_raw'] = f\"API Error: {e}\"\n",
    "        \n",
    "    # 4.4. Calculate Metrics\n",
    "    \n",
    "    df_results = df.dropna(subset=['predicted_stars']).copy()\n",
    "    if not df_results.empty:\n",
    "        accuracy = accuracy_score(df_results['actual_stars'].astype(int), df_results['predicted_stars'].astype(int))\n",
    "    else:\n",
    "        accuracy = 0.0\n",
    "\n",
    "    json_validity_rate = valid_json_count / len(df)\n",
    "    non_empty_explanation_count = df_results['explanation'].apply(lambda x: bool(x) and str(x).strip() != '').sum()\n",
    "    explanation_rate = non_empty_explanation_count / len(df)\n",
    "    \n",
    "    metrics = {\n",
    "        \"Approach\": prompt_name,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"JSON Validity Rate\": json_validity_rate,\n",
    "        \"Reliability (Non-Empty Explanation Rate)\": explanation_rate\n",
    "    }\n",
    "    \n",
    "    return metrics, df.copy()\n",
    "\n",
    "# --- 5. EXECUTE ALL APPROACHES AND COMPARE ---\n",
    "\n",
    "def main():\n",
    "    results_list = []\n",
    "    all_results = {}\n",
    "\n",
    "    # 5.1. Define Output Path\n",
    "    OUTPUT_DIR = Path(r\"C:\\Users\\saksh\\Downloads\\yelp.csv\")\n",
    "    OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Load sampled data (df_sampled must be loaded successfully in Section 2)\n",
    "    global df_sampled \n",
    "\n",
    "    if df_sampled.empty:\n",
    "        print(\"\\nCannot run evaluation: Sampled DataFrame is empty. Check your DATA_PATH.\")\n",
    "        return\n",
    "\n",
    "    for name, template in PROMPT_MAP.items():\n",
    "        print(f\"\\n--- Running {name} ---\")\n",
    "        \n",
    "        # Make a deep copy of the sampled data for each independent run\n",
    "        df_to_process = df_sampled[['text', 'actual_stars']].copy()\n",
    "        \n",
    "        metrics, df_output = classify_reviews_with_llm(df_to_process, template, name)\n",
    "        \n",
    "        results_list.append(metrics)\n",
    "        all_results[name] = df_output\n",
    "\n",
    "    # Create the final comparison table\n",
    "    df_comparison = pd.DataFrame(results_list).set_index('Approach')\n",
    "    \n",
    "    \n",
    "    # --- 6. SAVE AND PRINT OUTPUTS ---\n",
    "    \n",
    "    best_approach = df_comparison['Accuracy'].idxmax()\n",
    "    \n",
    "    # Combine all individual results dataframes for a single CSV export\n",
    "    combined_df = pd.concat([df.assign(Approach=name) for name, df in all_results.items()])\n",
    "    \n",
    "    # 6.1. Prepare the structure for JSON export\n",
    "    final_output_structure = {\n",
    "        \"metadata\": {\n",
    "            \"model_name\": MODEL_NAME,\n",
    "            \"sample_size\": SAMPLE_SIZE,\n",
    "            \"date\": datetime.now().isoformat()\n",
    "        },\n",
    "        \"comparison_metrics\": df_comparison.reset_index().to_dict(orient='records'),\n",
    "        \"best_approach\": best_approach,\n",
    "        # Convert the entire combined DataFrame to a list of dictionaries for JSON export\n",
    "        \"full_results_data\": combined_df.to_dict(orient='records')\n",
    "    }\n",
    "    \n",
    "    # 6.2. Save the final JSON analysis summary\n",
    "    json_file_path = OUTPUT_DIR / \"final_analysis_summary.json\"\n",
    "    with open(json_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(final_output_structure, f, indent=2, default=str)\n",
    "\n",
    "\n",
    "    # 6.3. Save the Markdown report and CSV (as in previous versions)\n",
    "    markdown_content = []\n",
    "    markdown_content.append(\"# Gemini Prompt Comparison Analysis\\n\")\n",
    "    markdown_content.append(\"## 1. Metric Comparison\\n\")\n",
    "    markdown_content.append(df_comparison.to_markdown())\n",
    "    markdown_content.append(\"\\n\\n---\\n\")\n",
    "\n",
    "    markdown_content.append(f\"## 2. Detailed Results for Best Approach: {best_approach}\\n\")\n",
    "    markdown_content.append(\"\\nExample 5 Predictions from the Best Approach:\\n\")\n",
    "    report_df = all_results[best_approach][['text', 'actual_stars', 'predicted_stars', 'explanation']].head()\n",
    "    markdown_content.append(report_df.to_markdown(index=False))\n",
    "\n",
    "    report_file_path = OUTPUT_DIR / \"analysis_report.md\"\n",
    "    with open(report_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.writelines(markdown_content)\n",
    "        \n",
    "    full_df_path = OUTPUT_DIR / \"full_predictions_and_metrics.csv\"\n",
    "    combined_df.to_csv(full_df_path, index=False)\n",
    "    \n",
    "    \n",
    "    # 6.4. Print confirmation to console\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"           TASK 1: PROMPT COMPARISON RESULTS\")\n",
    "    print(\"=\"*50)\n",
    "    print(df_comparison.to_markdown())\n",
    "    print(\"\\n--- RESULTS SAVED ---\")\n",
    "    print(f\"✓ JSON Analysis Summary: {json_file_path.resolve()}\")\n",
    "    print(f\"✓ Comparison Report (Markdown): {report_file_path.resolve()}\")\n",
    "    print(f\"✓ Full Data (CSV): {full_df_path.resolve()}\")\n",
    "    print(\"---------------------\\n\")\n",
    "\n",
    "    return df_comparison\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
